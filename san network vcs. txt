Excellent question â€” the slow failover (basculement lent) you observed could indeed come from either hardware (server health) or network/storage issues, and both often appear together in Veritas Cluster incidents.

Hereâ€™s how to interpret it clearly ðŸ‘‡


---

ðŸ§  Two Main Categories of Causes

ðŸ©º 1. â€œHealthâ€ (Server / System) Problems

These are local resource issues on the node itself.

Typical signs:

High CPU usage or memory exhaustion (paging, page faults, swap spikes).

Disk I/O saturation (slow local filesystem or overloaded HBA).

Application or process hang (agent timeout in Veritas).

Kernel delays or process stuck in uninterruptible sleep (D state).


How it affects failover:

The VCS agent takes too long to offline or clean the resource.

MonitorTimeout is reached â†’ resource marked FAULTED.

The next node waits for lock release â†’ slow group switch.


Verification:

sar -u 1 10               # CPU usage
sar -B -W -r -f /var/log/sa/sa15   # Paging & swap
vmstat 1 10               # Processes waiting on I/O
vxprint -ht               # DiskGroup states


---

ðŸŒ 2. Network / Storage (SAN) Problems

These are infrastructure issues affecting cluster communication or disk access.

Typical signs:

Errors in LLT/GAB logs (heartbeat lost or flapping link).

DMP (Veritas Dynamic Multipathing) reports path failed / restored.

Slow SAN response â†’ DiskGroup offline delay.

NFS or shared volume latency.


How it affects failover:

VCS may see the heartbeat drop â†’ suspects node failure â†’ triggers failover.

DiskGroup unmount or deport takes long â†’ delayed start on the new node.

Sometimes a â€œsplit brainâ€ situation is avoided only after fencing resolves â†’ extra delay.


Verification:

gabconfig -a
lltstat -nvv
vxdmpadm iostat show interval=1 count=5
grep -Ei 'vxdmp|I/O error|path failed' /var/log/messages


---

ðŸ“Š In Your Case (Based on Graphs)

12:00 â†’ memory + paging explosion â†’ health side (RAM pressure, paging due to restart).

However, such restarts are usually triggered by a storage or heartbeat problem (network side).


So:

> Primary trigger: Network or SAN transient failure (path/heartbeat loss).
Secondary effect: Application restart caused heavy paging and slow recovery (system health degradation).




---

ðŸ§° Recommendations

Area	Action

Network (LLT/GAB)	Check redundancy (at least 2 LLT links, separate NICs/VLANs). Look for packet loss, MTU mismatch, spanning-tree delays.
Storage (DMP)	Validate all paths online, no â€œpath fail/restoreâ€ events. Tune dmp_pathswitch_blktime and SAN timeouts.
System Health	Increase RAM if swap used heavily. Monitor CPU steal/wait (%wa). Optimize app startup memory.
Cluster Config	Tune Veritas parameters: MonitorTimeout, FaultOnMonitorTimeout, RestartLimit, OnlineRetryLimit. Enable fencing (SCSI-3).



---

âœ… Simplified RCA Summary (English)

> The failover delay around 12:00 was caused by a transient storage or heartbeat network disturbance in the Veritas Cluster, which triggered a service group failover.
When the application restarted on the new node, the system experienced memory paging and high page faults, leading to temporary performance degradation.

Root cause: Intermittent SAN or heartbeat latency, combined with high memory load during restart.
Corrective actions: Verify LLT/GAB link stability, DMP path latency, increase RAM, and adjust Veritas agent timeouts.




---

Would you like me to generate a formal RCA document (in English or French) you can send to management or include in a ticket?