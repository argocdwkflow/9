Excellent question — the slow failover (basculement lent) you observed could indeed come from either hardware (server health) or network/storage issues, and both often appear together in Veritas Cluster incidents.

Here’s how to interpret it clearly 👇


---

🧠 Two Main Categories of Causes

🩺 1. “Health” (Server / System) Problems

These are local resource issues on the node itself.

Typical signs:

High CPU usage or memory exhaustion (paging, page faults, swap spikes).

Disk I/O saturation (slow local filesystem or overloaded HBA).

Application or process hang (agent timeout in Veritas).

Kernel delays or process stuck in uninterruptible sleep (D state).


How it affects failover:

The VCS agent takes too long to offline or clean the resource.

MonitorTimeout is reached → resource marked FAULTED.

The next node waits for lock release → slow group switch.


Verification:

sar -u 1 10               # CPU usage
sar -B -W -r -f /var/log/sa/sa15   # Paging & swap
vmstat 1 10               # Processes waiting on I/O
vxprint -ht               # DiskGroup states


---

🌐 2. Network / Storage (SAN) Problems

These are infrastructure issues affecting cluster communication or disk access.

Typical signs:

Errors in LLT/GAB logs (heartbeat lost or flapping link).

DMP (Veritas Dynamic Multipathing) reports path failed / restored.

Slow SAN response → DiskGroup offline delay.

NFS or shared volume latency.


How it affects failover:

VCS may see the heartbeat drop → suspects node failure → triggers failover.

DiskGroup unmount or deport takes long → delayed start on the new node.

Sometimes a “split brain” situation is avoided only after fencing resolves → extra delay.


Verification:

gabconfig -a
lltstat -nvv
vxdmpadm iostat show interval=1 count=5
grep -Ei 'vxdmp|I/O error|path failed' /var/log/messages


---

📊 In Your Case (Based on Graphs)

12:00 → memory + paging explosion → health side (RAM pressure, paging due to restart).

However, such restarts are usually triggered by a storage or heartbeat problem (network side).


So:

> Primary trigger: Network or SAN transient failure (path/heartbeat loss).
Secondary effect: Application restart caused heavy paging and slow recovery (system health degradation).




---

🧰 Recommendations

Area	Action

Network (LLT/GAB)	Check redundancy (at least 2 LLT links, separate NICs/VLANs). Look for packet loss, MTU mismatch, spanning-tree delays.
Storage (DMP)	Validate all paths online, no “path fail/restore” events. Tune dmp_pathswitch_blktime and SAN timeouts.
System Health	Increase RAM if swap used heavily. Monitor CPU steal/wait (%wa). Optimize app startup memory.
Cluster Config	Tune Veritas parameters: MonitorTimeout, FaultOnMonitorTimeout, RestartLimit, OnlineRetryLimit. Enable fencing (SCSI-3).



---

✅ Simplified RCA Summary (English)

> The failover delay around 12:00 was caused by a transient storage or heartbeat network disturbance in the Veritas Cluster, which triggered a service group failover.
When the application restarted on the new node, the system experienced memory paging and high page faults, leading to temporary performance degradation.

Root cause: Intermittent SAN or heartbeat latency, combined with high memory load during restart.
Corrective actions: Verify LLT/GAB link stability, DMP path latency, increase RAM, and adjust Veritas agent timeouts.




---

Would you like me to generate a formal RCA document (in English or French) you can send to management or include in a ticket?